training_args:
  predict_with_generate: true
  per_device_train_batch_size: 24
  per_device_eval_batch_size: 48
  gradient_accumulation_steps: 4
  # gradient_checkpointing: true
  num_train_epochs: 1
  learning_rate: 5.0e-5
  output_dir: checkpoints
  logging_dir: logs
  resume_from_checkpoint: checkpoints
  logging_strategy: steps
  logging_steps: 10
  log_level: info
  save_strategy: steps
  save_total_limit: 8
  save_steps: 200
  evaluation_strategy: steps
  eval_steps: 200
  dataloader_num_workers: 5
  optim: adamw_torch
  lr_scheduler_type: polynomial
  warmup_steps: 768
  weight_decay: 1.0e-2
  load_best_model_at_end: true
  metric_for_best_model: hwdb_loss
  greater_is_better: false
  dataloader_pin_memory: true

processor_args:
  size: 384
  resample: 2
  image_mean: [0.485, 0.456, 0.406]
  image_std: [0.229, 0.224, 0.225]

encoder: facebook/deit-base-distilled-patch16-384
decoder: hfl/chinese-macbert-base

datasets:
  train:
    mode: online
    arbitrary_size: 16000000
