{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-niandai/1.png\t华俊龙苦笑着说：文晋，这不公平：你的\n",
      "章文晋就走来了。他告诉大宗说：刚才我从廖\n",
      "test-niandai/2.png\t都教一个班，我却教两个班，还是二年级，未\n",
      "教不好，才是我们的责任。今天就讲到这里为\n",
      "test-niandai/3.png\t免有失公道吧！\n",
      "家在这片“死亡之海”中视角不同的观察，导致了一个世纪的学术争\n",
      "test-niandai/4.png\t李文晋笑着说：“是不公平，这分工作总得\n",
      "高原发现24个新物种，其中包括一种能发荧光的紫色蛙。\n",
      "test-niandai/5.png\t有人去干吧？你说，该给谁干呢？我没说一句，\n",
      "是手表行业中首家发布客服体系的品牌，不仅\n",
      "test-niandai/6.png\t他们就告诉我：这是教务科的安排。廖组长还\n",
      "你到底是谁？\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model_pth = \"models/tang-syn-3.0-epoch-1/\"\n",
    "# model_pth = \"models/trocr-chinese-handwritten\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_pth)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pth)\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "truth = \"\"\"\n",
    "test-niandai/1.png\t华俊龙苦笑着说：文晋，这不公平：你的\n",
    "test-niandai/2.png\t都教一个班，我却教两个班，还是二年级，未\n",
    "test-niandai/3.png\t免有失公道吧！\n",
    "test-niandai/4.png\t李文晋笑着说：“是不公平，这分工作总得\n",
    "test-niandai/5.png\t有人去干吧？你说，该给谁干呢？我没说一句，\n",
    "test-niandai/6.png\t他们就告诉我：这是教务科的安排。廖组长还\n",
    "test-niandai/7.png\t说：“你们一起来的华俊龙老师，领导者得他起，\n",
    "test-niandai/8.png\t就把重担交给他了。”\n",
    "test-niandai/9.png\t张厚生笑着说：“这叫能者多劳，你就担起\n",
    "test-niandai/10.png\t这个危险吧，有什么困难，我们都会帮忙的。”\n",
    "test-niandai/11.png\t周宏义也说：“我们刚来，学校安排工作，\n",
    "test-niandai/12.png\t我们还是愉快地接受下来，不说什么为好。当\n",
    "test-niandai/13.png\t然俊龙一个人任二年级的课，而且教两个班，\n",
    "test-niandai/14.png\t比我们辛苦许多，从另一方面说，也是学校\n",
    "test-niandai/15.png\t对你的重视，不过，无论怎么说，还是辛苦老\n",
    "\"\"\".strip().split(\"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "华俊龙苦笑着说：文晋，这不公平：你的\n",
    "都教一个跳，我有教两个教：还是二年级，半\n",
    "免有失公道理吧！\n",
    "李文晋笑着说：是不公平·这分二作总得\n",
    "有人去干吧？你说，该经纶干呢？我没说一句，\n",
    "他的就告诉出：这是教育科的安排心疼但长还\n",
    "说：你们一起来自华俊龙考师：领导者得他想\n",
    "孔包含粗皮线他3。\n",
    "张厚生笑着说：这种稻香变芽，你就把老\n",
    "这个危险吧·有什么困难·我们都会帮忙好。\n",
    "周宏义也说：我们刚翠·李松姿那二作\n",
    "划的还是瞬快地接受下来：不说什么为好。当\n",
    "丝缠在一个人任二年级好课，而且教两个班\n",
    "形式仍会幸苦许多：从另一方面说：也是学校\n",
    "对你的重视：不过。无论怎么说，还是辛苦\n",
    "\"\"\"\n",
    "\n",
    "# test_dir = \"dataset/data/test-niandai\"\n",
    "test_dir = \"samples/images\"\n",
    "for file in os.listdir(test_dir):\n",
    "    if not file.endswith((\".jpg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    print(truth.pop(0))\n",
    "\n",
    "    image = Image.open(os.path.join(test_dir, file)).convert(\"RGB\")\n",
    "\n",
    "    pixel_values = (\n",
    "        processor(image, return_tensors=\"pt\").pixel_values).to(device)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = tokenizer.decode(\n",
    "        generated_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text.replace(\" \", \"\"))\n",
    "\n",
    "# text = \"李文晋就走来了。他告诉大家说：“刚才我从廖\"\n",
    "# image = Image.open(os.path.join(\"sample_imgs\", \"0.png\")).convert(\"RGB\")\n",
    "# # print(processor.image_processor.to_dict())\n",
    "# pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "# labels = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "# # outputs = model(pixel_values, labels=labels)\n",
    "# outputs = model.generate(pixel_values)\n",
    "# generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "# print(labels, outputs, generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 872,\n",
       " 3221,\n",
       " 2769,\n",
       " 4638,\n",
       " 2207,\n",
       " 5741,\n",
       " 3362,\n",
       " 872,\n",
       " 3221,\n",
       " 2769,\n",
       " 4638,\n",
       " 1920,\n",
       " 5741,\n",
       " 107,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model_pth = \"models/tang-syn-2.0-checkpoint-374000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pth)\n",
    "\n",
    "input_ids = tokenizer(\"你是我的小苹果 你是我的大苹\\\"““”‘’\").input_ids\n",
    "input_ids\n",
    "# tokenizer.decode(input_ids)\n",
    "# tokenizer.decode([2, 17, 7158, 8884, 8884, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/rocm-user/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--wer/85bee9e4216a78bb09b2d0d500f6af5c23da58f9210e661add540f5df6630fcd (last modified on Sat May 20 10:39:44 2023) since it couldn't be found locally at evaluate-metric--wer, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m     wer \u001b[39m=\u001b[39m wer_metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpred_str, references\u001b[39m=\u001b[39mlabels_str)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mcer\u001b[39m\u001b[39m\"\u001b[39m: cer, \u001b[39m\"\u001b[39m\u001b[39mwer\u001b[39m\u001b[39m\"\u001b[39m: wer}\n\u001b[1;32m     26\u001b[0m pred \u001b[39m=\u001b[39m {\n\u001b[0;32m---> 27\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39marray([outputs]),\n\u001b[1;32m     28\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlabel_ids\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39marray([labels])\n\u001b[1;32m     29\u001b[0m }\n\u001b[1;32m     31\u001b[0m compute_metrics(pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.get(\"label_ids\")\n",
    "    pred_ids = pred.get(\"predictions\")\n",
    "\n",
    "    # Predictions come with space between\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    print(labels_str, pred_str)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=labels_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=labels_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}\n",
    "\n",
    "\n",
    "pred = {\n",
    "    \"predictions\": np.array([outputs]),\n",
    "    \"label_ids\": np.array([labels])\n",
    "}\n",
    "\n",
    "compute_metrics(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dotin13/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dotin13/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hwdb2.0_4k.tsv\n",
      "Processing hwdb_ic13_3k.tsv\n",
      "Processing hw_chinese_17k.tsv\n",
      "Processing hwdb2.2_3k.tsv\n",
      "Processing hwdb2.1_3k.tsv\n",
      "Processing hwdb_ic13_val_5k.tsv\n",
      "Number of validation examples: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n",
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.io as io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from data_aug import build_data_aug\n",
    "from torch.utils.data import Subset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"checkpoints/checkpoint-360000/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_dir,\n",
    "                 labels_dir,\n",
    "                 transform,\n",
    "                 processor,\n",
    "                 tokenizer,\n",
    "                 mode=\"train\",\n",
    "                 max_target_length=32,\n",
    "                 device=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.processor = processor\n",
    "        self.mode = mode\n",
    "        self.max_target_length = max_target_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = self.build_df()\n",
    "        self.df_len = len(self.df)\n",
    "        self.arbitrary_len = 12000000\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arbitrary_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Thirty percent of the time, use existing dataset\n",
    "        if idx < self.df_len:\n",
    "            text = self.df['text'][idx]\n",
    "            # get file name + text\n",
    "            file_name = self.df[\"file_name\"][idx]\n",
    "            # prepare image (i.e. resize + normalize)\n",
    "            image = Image.open(path.join(self.dataset_dir,\n",
    "                                         file_name)).convert(\"RGB\")\n",
    "        # 70% percent of the time, use online generated data\n",
    "        else:\n",
    "            text_idx = int(idx / self.arbitrary_len * (self.df_len - 1))\n",
    "            text = self.df['text'][text_idx]\n",
    "            bgr_image = synthesize(text)\n",
    "            rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(rgb_image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # Remove spaces from text, as only the tang-syn version 1 had spaces before text\n",
    "        text = text.strip()\n",
    "        labels = self.tokenizer(text,\n",
    "                                padding=\"max_length\",\n",
    "                                stride=32,\n",
    "                                truncation=True,\n",
    "                                max_length=self.max_target_length).input_ids\n",
    "\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [\n",
    "            label if label != self.tokenizer.pad_token_id else -100\n",
    "            for label in labels\n",
    "        ]\n",
    "\n",
    "        encoding = {\n",
    "            \"pixel_values\": pixel_values.squeeze(),\n",
    "            \"labels\": torch.tensor(labels)\n",
    "        }\n",
    "        return encoding\n",
    "\n",
    "    def build_df(self):\n",
    "        li = []\n",
    "        for root, _dirs, files in os.walk(self.labels_dir):\n",
    "            for file in files:  # Loop through the dataset tsvfiles\n",
    "                if not file.endswith(\".tsv\"):\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing {file}\")\n",
    "                li.append(\n",
    "                    pd.read_table(path.join(root, file),\n",
    "                                  names=[\"file_name\", \"text\"]))\n",
    "\n",
    "        return pd.concat(li, axis=0, ignore_index=True)\n",
    "    \n",
    "\n",
    "dataset_dir = 'dataset/data'\n",
    "max_length = 64\n",
    "\n",
    "# Define the number of samples to keep in eval dataset\n",
    "num_samples = 200\n",
    "\n",
    "eval_dataset = OCRDataset(\n",
    "    dataset_dir=dataset_dir,\n",
    "    labels_dir=\"dataset/labels/test\",\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    mode=\"eval\",\n",
    "    transform=None,\n",
    "    max_target_length=max_length\n",
    ")\n",
    "\n",
    "# Create a random subset of the dataset\n",
    "subset_indices = torch.randperm(len(eval_dataset))[:num_samples]\n",
    "eval_dataset = Subset(eval_dataset, subset_indices.tolist())\n",
    "\n",
    "print(\"Number of validation examples:\", len(eval_dataset))\n",
    "\n",
    "\n",
    "def init_trainer(model, tokenizer, compute_metrics, train_dataset,\n",
    "                 eval_dataset):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        predict_with_generate=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        fp16=True,\n",
    "        learning_rate=4e-5,\n",
    "        output_dir=\"./checkpoints\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=5,\n",
    "        save_steps=10000,\n",
    "        eval_steps=10000,\n",
    "        resume_from_checkpoint=\"./checkpoints/\",\n",
    "        dataloader_num_workers=10)\n",
    "\n",
    "    # instantiate trainer\n",
    "    return Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "trainer = init_trainer(model, tokenizer, compute_metrics, eval_dataset, eval_dataset)\n",
    "\n",
    "eval_result = None\n",
    "with torch.no_grad():\n",
    "    eval_result = trainer.evaluate(eval_dataset, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3815649747848511,\n",
       " 'eval_cer': 0.09589737076082815,\n",
       " 'eval_wer': 0.16901408450704225,\n",
       " 'eval_runtime': 23.1828,\n",
       " 'eval_samples_per_second': 8.627,\n",
       " 'eval_steps_per_second': 0.561}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "from pstats import SortKey\n",
    "p = pstats.Stats('0.log')\n",
    "p.strip_dirs().sort_stats(\"cumtime\").print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup fonts that does not support cjk\n",
    "\n",
    "import os\n",
    "import pygame\n",
    "import pygame.freetype\n",
    "from fontTools.ttLib import TTFont\n",
    "\n",
    "pygame.freetype.init()\n",
    "\n",
    "\n",
    "def get_font_names(font_path):\n",
    "    font = TTFont(font_path)\n",
    "    names = []\n",
    "    for record in font['name'].names:\n",
    "        if record.nameID == 4:\n",
    "            if record.getEncoding() == \"x_mac_simp_chinese_ttx\":\n",
    "                names.append(record.string.decode('gbk'))\n",
    "            else:\n",
    "                names.append(record.toStr())\n",
    "\n",
    "    return names\n",
    "\n",
    "\n",
    "for file in os.listdir(\"fonts\"):\n",
    "    if file.lower().endswith((\".ttf\", \".otf\")):\n",
    "        # font = pygame.freetype.Font(f\"fonts/{file}\", size=10)\n",
    "        # metrics = font.get_metrics(\"你是我的小苹果，我是你的大苹果！\")\n",
    "\n",
    "        # invalid_metrics = not all(metrics)\n",
    "\n",
    "        names = get_font_names(f\"fonts/{file}\")\n",
    "        invalid_type = any(\"篆\" in font_name or (\n",
    "            \"繁\" in font_name and \"简\" not in font_name) for font_name in names)\n",
    "\n",
    "        if invalid_type:\n",
    "            # print(names)\n",
    "            os.rename(f\"fonts/{file}\", f\"fonts/archives/{file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
