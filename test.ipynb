{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/home/dotin13/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李 文 晋 就 走 来 了 。 他 告 诉 大 家 说 ： 啊 才 我 从 瘦\n",
      "教 不 好 ， 才 是 我 仍 好 惹 任 · 今 天 礼 讲 到 这 里 为\n",
      "育 原 发 现 24 个 新 物 种 ， 其 中 包 括 一 种 能 发 荧 光 的 紫 色 蛙 。\n",
      "家 在 这 片 死 亡 之 海 中 视 角 不 同 的 观 察 ， 导 致 了 一 个 世 纪 的 学 术 争 论 ，\n",
      "是 可 表 行 业 内 知 名 度 和 市 场 占 有 份 额 。 而 其\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"checkpoints/checkpoint-310000/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "for file in os.listdir(\"sample_imgs\"):\n",
    "    if not file.endswith((\".jpg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    image = Image.open(os.path.join(\"sample_imgs\", file)).convert(\"RGB\")\n",
    "\n",
    "    pixel_values = (processor(image, return_tensors=\"pt\").pixel_values).to(device)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import BasicTokenizer\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # Predictions come with space between\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=labels_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=labels_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}\n",
    "\n",
    "# pred = {\n",
    "#     \"predictions\": np.array([[ 101,  101, 3330, 3152, 3232, 2218, 6624, 1092,  749,  511,  800, 1440, 6401, 1920, 2157, 6432, 8038, 1157, 2798, 2769,  794, 2445,  102]]),\n",
    "#     \"label_ids\": np.array([[101, 3330, 3152, 3232, 2218, 1139, 3341, 749, 511, 800, 1440, 6401, 1920, 2157, 6432, 8038, 1157, 2798, 2769, 794, 2445, 102]])\n",
    "# }\n",
    "\n",
    "# compute_metrics(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hwdb2.0_4k.tsv\n",
      "Processing hwdb_ic13_3k.tsv\n",
      "Processing hw_chinese_17k.tsv\n",
      "Processing hwdb2.2_3k.tsv\n",
      "Processing hwdb2.1_3k.tsv\n",
      "Processing hwdb_ic13_val_5k.tsv\n",
      "Number of validation examples: 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.io as io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from data_aug import build_data_aug\n",
    "from torch.utils.data import Subset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"checkpoints/checkpoint-310000/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, labels_dir, transform, processor, tokenizer, mode=\"train\", max_target_length=32, device=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.processor = processor\n",
    "        self.mode = mode\n",
    "        self.max_target_length = max_target_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = self.build_df()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text\n",
    "        file_name = self.df[\"file_name\"][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(path.join(self.dataset_dir, file_name)).convert(\"RGB\")\n",
    "        if self.mode == \"train\" and self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        labels = self.tokenizer(text, padding=\"max_length\",\n",
    "                                stride=32,\n",
    "                                truncation=True,\n",
    "                                max_length=self.max_target_length).input_ids\n",
    "        \n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding\n",
    "\n",
    "    def build_df(self):\n",
    "        li = []\n",
    "        for root, dirs, files in os.walk(self.labels_dir):\n",
    "            for file in files:  # Loop through the dataset tsvfiles\n",
    "                if not file.endswith(\".tsv\"):\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing {file}\")\n",
    "                li.append(pd.read_table(path.join(root, file),\n",
    "                          names=[\"file_name\", \"text\"]))\n",
    "\n",
    "        return pd.concat(li, axis=0, ignore_index=True)\n",
    "    \n",
    "\n",
    "dataset_dir = 'dataset/data'\n",
    "max_length = 64\n",
    "\n",
    "# Define the number of samples to keep in eval dataset\n",
    "num_samples = 200\n",
    "\n",
    "eval_dataset = OCRDataset(\n",
    "    dataset_dir=dataset_dir,\n",
    "    labels_dir=\"dataset/labels/test\",\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    mode=\"eval\",\n",
    "    transform=None,\n",
    "    max_target_length=max_length\n",
    ")\n",
    "\n",
    "# Create a random subset of the dataset\n",
    "subset_indices = torch.randperm(len(eval_dataset))[:num_samples]\n",
    "eval_dataset = Subset(eval_dataset, subset_indices.tolist())\n",
    "\n",
    "print(\"Number of validation examples:\", len(eval_dataset))\n",
    "\n",
    "\n",
    "def init_trainer(model, tokenizer, compute_metrics, train_dataset,\n",
    "                 eval_dataset):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        predict_with_generate=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        fp16=True,\n",
    "        learning_rate=4e-5,\n",
    "        output_dir=\"./checkpoints\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=5,\n",
    "        save_steps=10000,\n",
    "        eval_steps=10000,\n",
    "        resume_from_checkpoint=\"./checkpoints/\",\n",
    "        dataloader_num_workers=10)\n",
    "\n",
    "    # instantiate trainer\n",
    "    return Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "trainer = init_trainer(model, tokenizer, compute_metrics, eval_dataset, eval_dataset)\n",
    "\n",
    "eval_result = None\n",
    "with torch.no_grad():\n",
    "    eval_result = trainer.evaluate(eval_dataset, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2817695736885071,\n",
       " 'eval_cer': 0.06323755772190867,\n",
       " 'eval_wer': 0.11390157280568239,\n",
       " 'eval_runtime': 21.6212,\n",
       " 'eval_samples_per_second': 9.25,\n",
       " 'eval_steps_per_second': 0.601}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n",
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n",
      "Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n",
      "Unable to measure model FLOPs due to error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
      "Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00, 212.54it/s]\n",
      "Measuring inference for batch_size=1: 100%|██████████| 10/10 [00:00<00:00, 231.46it/s]\n",
      "Unable to measure energy consumption. Device must be a NVIDIA Jetson.\n",
      "Warming up with batch_size=64: 100%|██████████| 1/1 [00:00<00:00, 27.10it/s]\n",
      "Measuring inference for batch_size=64: 100%|██████████| 10/10 [00:00<00:00, 27.00it/s]\n",
      "Unable to measure energy consumption. Device must be a NVIDIA Jetson.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Unable to measure model FLOPs due to error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
      "Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00, 93.60it/s]\n",
      "Measuring inference for batch_size=1: 100%|██████████| 10/10 [00:00<00:00, 99.21it/s]\n",
      "Unable to measure energy consumption. Device must be a NVIDIA Jetson.\n",
      "Warming up with batch_size=64: 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]\n",
      "Measuring inference for batch_size=64: 100%|██████████| 10/10 [00:01<00:00,  6.92it/s]\n",
      "Unable to measure energy consumption. Device must be a NVIDIA Jetson.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Unable to measure model FLOPs due to error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
      "Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00, 119.08it/s]\n",
      "Measuring inference for batch_size=1: 100%|██████████| 10/10 [00:00<00:00, 124.15it/s]\n",
      "Unable to measure energy consumption. Device must be a NVIDIA Jetson.\n",
      "Warming up with batch_size=64: 100%|██████████| 1/1 [00:00<00:00, 27.94it/s]\n",
      "Measuring inference for batch_size=64: 100%|██████████| 10/10 [00:00<00:00, 27.06it/s]\n",
      "Unable to measure energy consumption. Device must be a NVIDIA Jetson.\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "from torchvision.models import efficientnet_b0, vit_l_16, densenet161, regnet_y_1_6gf\n",
    "from pytorch_benchmark import benchmark\n",
    "\n",
    "print(\"################################################################\")\n",
    "model = efficientnet_b0().cuda()\n",
    "sample = torch.randn(64, 3, 224, 224)  # (B, C, H, W)\n",
    "results = benchmark(model, sample, num_runs=10)\n",
    "\n",
    "print(\"################################################################\")\n",
    "model2 =  densenet161().cuda()\n",
    "sample2 = torch.randn(64, 3, 224, 224)  # (B, C, H, W)\n",
    "results2 = benchmark(model2, sample2, num_runs=10)\n",
    "\n",
    "print(\"################################################################\")\n",
    "model3 =  regnet_y_1_6gf().cuda()\n",
    "sample3 = torch.randn(64, 3, 224, 224)  # (B, C, H, W)\n",
    "results3 = benchmark(model3, sample3, num_runs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
