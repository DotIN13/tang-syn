{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e191846-8c83-442d-83d1-80a556d7303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_TRAINING = False\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pynvml import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9c225-b422-47c2-a025-6038bdde44b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "if FULL_TRAINING:\n",
    "    vision_hf_model = 'facebook/deit-base-distilled-patch16-384'\n",
    "    nlp_hf_model = \"hfl/chinese-macbert-base\"\n",
    "    \n",
    "    # Reference: https://github.com/huggingface/transformers/issues/15823\n",
    "    # initialize the encoder from a pretrained ViT and the decoder from a pretrained BERT model. \n",
    "    # Note that the cross-attention layers will be randomly initialized, and need to be fine-tuned on a downstream dataset\n",
    "    model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(vision_hf_model, nlp_hf_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(nlp_hf_model)\n",
    "else:\n",
    "    trocr_model = 'models/epoch-1/'\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(trocr_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(trocr_model)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c0868-fcdd-4278-9766-9784770ec5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "# train_df.reset_index(drop=True, inplace=True)\n",
    "# test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fcf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.io as io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, labels_dir, transform, processor, tokenizer, mode=\"train\", max_target_length=32, device=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.processor = processor\n",
    "        self.mode = mode\n",
    "        self.max_target_length = max_target_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = self.build_df()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text\n",
    "        file_name = self.df[\"file_name\"][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(path.join(self.dataset_dir, file_name)).convert(\"RGB\")\n",
    "        if self.mode == \"train\" and self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        labels = self.tokenizer(text, padding=\"max_length\",\n",
    "                                stride=32,\n",
    "                                truncation=True,\n",
    "                                max_length=self.max_target_length).input_ids\n",
    "        \n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding\n",
    "\n",
    "    def build_df(self):\n",
    "        li = []\n",
    "        for root, dirs, files in os.walk(self.labels_dir):\n",
    "            for file in files:  # Loop through the dataset tsvfiles\n",
    "                if not file.endswith(\".tsv\"):\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing {file}\")\n",
    "                li.append(pd.read_table(path.join(root, file),\n",
    "                          names=[\"file_name\", \"text\"]))\n",
    "\n",
    "        return pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9099e8-b0b6-447f-aa91-076db09d5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "from data_aug import build_data_aug\n",
    "from torch.utils.data import Subset, random_split\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "dataset_dir = 'dataset/data'\n",
    "max_length = 64\n",
    "\n",
    "train_dataset = OCRDataset(\n",
    "    dataset_dir=dataset_dir,\n",
    "    labels_dir=\"dataset/labels/train\",\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    mode=\"train\",\n",
    "    transform=build_data_aug((64, 1024), \"train\"),\n",
    "    max_target_length=max_length\n",
    ")\n",
    "\n",
    "# Define the number of samples to keep in eval dataset\n",
    "num_samples = 100\n",
    "\n",
    "eval_dataset = OCRDataset(\n",
    "    dataset_dir=dataset_dir,\n",
    "    labels_dir=\"dataset/labels/test\",\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    mode=\"eval\",\n",
    "    transform=None,\n",
    "    max_target_length=max_length\n",
    ")\n",
    "\n",
    "# Create a random subset of the dataset\n",
    "subset_indices = torch.randperm(len(eval_dataset))[:num_samples]\n",
    "eval_dataset = Subset(eval_dataset, subset_indices.tolist())\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddb3bf-d83c-47a5-b0cc-93dda0b71e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.max_length = max_length\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a77f4-08a0-49a3-b590-b4155aa05199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    learning_rate=4e-5,\n",
    "    output_dir=\"./checkpoints\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=5,\n",
    "    save_steps=10000,\n",
    "    eval_steps=10000,\n",
    "    resume_from_checkpoint=\"./checkpoints/\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0b915e5-d73e-47df-83c3-236b9daccd31",
   "metadata": {},
   "source": [
    "문자 오류율(CER; Character Error Rate)과 단어 오류율(WER; Word Error Rate) 지표로 모델을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e798c419-d950-4f6e-bcfa-c247162f5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee0cad-48c8-49f8-a8ae-0e86efa57d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=labels_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=labels_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b42d6-c389-4719-9e10-c67e486e6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe0990-a6fa-4caf-83a1-578946adc51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54ce86-af28-4323-a9b8-3074ddd999fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.train(\"models/epoch-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model:\n",
    "    del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f9ebd-3da8-4041-8c15-87c7e05de9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_TRAINING:\n",
    "    steps = []\n",
    "    losses = []\n",
    "    for obj in trainer.state.log_history:\n",
    "        if obj.get(\"step\") and obj.get(\"loss\"):\n",
    "            steps.append(obj['step'])\n",
    "            losses.append(obj['loss'])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    f = plt.figure(figsize=(12,6))\n",
    "    plt.plot(steps, losses)\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('training loss')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1602f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SCALER_NAME = \"scaler.pt\"\n",
    "SCHEDULER_NAME = \"scheduler.pt\"\n",
    "OPTIMIZER_NAME = \"optimizer.pt\"\n",
    "\n",
    "output_dir = \"./models/epoch-1\"\n",
    "\n",
    "trainer.save_model(output_dir=output_dir)\n",
    "trainer.save_state()\n",
    "\n",
    "torch.save(trainer.scaler.state_dict(), os.path.join(output_dir, SCALER_NAME))\n",
    "torch.save(trainer.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n",
    "torch.save(trainer.scaler.state_dict(), os.path.join(output_dir, SCALER_NAME))\n",
    "torch.save(trainer.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))\n",
    "# Single gpu\n",
    "rng_states = {\n",
    "    \"python\": random.getstate(),\n",
    "    \"numpy\": np.random.get_state(),\n",
    "    \"cpu\": torch.random.get_rng_state(),\n",
    "    \"cuda\": torch.cuda.random.get_rng_state_all()\n",
    "}\n",
    "torch.save(rng_states, os.path.join(output_dir, \"rng_state.pth\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b652e7b0-3ac4-4f7a-bd43-6e708faf032b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Evaluation and Inference\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e24c9-76a0-4862-96cb-6c13a1c7e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, random_split\n",
    "\n",
    "# Define the number of samples you want to keep\n",
    "num_samples = 100\n",
    "\n",
    "# Create a random subset of the dataset\n",
    "subset_indices = torch.randperm(len(eval_dataset))[:num_samples]\n",
    "subset = Subset(eval_dataset, subset_indices.tolist())\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    eval_result = trainer.evaluate(subset, max_length=64)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308954b-92aa-40ba-837e-6c5755363401",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045a0bb-ba7e-4834-a061-0161d054a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "sample_img_paths = glob.glob('sample_imgs/*.png')\n",
    "img_idx = np.random.randint(len(sample_img_paths))\n",
    "image = Image.open(sample_img_paths[img_idx]).convert(\"RGB\")\n",
    "#img_idx = np.random.randint(len(eval_dataset))\n",
    "#image = Image.open(eval_dataset.dataset_dir + train_df['file_name'][img_idx])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43821c-a41c-45fc-9eab-36a1de873271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "device = torch.device('cuda')\n",
    "\n",
    "pixel_values = (processor(image, return_tensors=\"pt\").pixel_values).to(device)\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] \n",
    "generated_ids, generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([ 101,  101, 3330, 3152, 3232, 2821, 6624, 1092,  749,  511,  800, 1440,\n",
    "          6401, 1920, 2157, 6432, 8038, 1157, 2798, 2769,  794, 3178,  102])\n",
    "# generated_text = tokenizer.convert_ids_to_tokens(labels, skip_special_tokens=True)\n",
    "generated_text = tokenizer.decode(labels, clean_up_tokenization_spaces=True, skip_special_tokens=False)\n",
    "generated_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a3b6c3e-9c94-4873-9277-aba07069f3df",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Clean up\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310923c4-c85f-4f3e-9084-ff383a4f39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c985514-d040-455e-8ad2-52c1faf5c0b4",
   "metadata": {},
   "source": [
    "## (Optional) Upload to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d2e47-231f-4dbd-8762-a887fd2d6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed964996-6d2d-42e8-a944-7f07c5f86fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed79bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # repo\n",
    "# MODEL_SAVE_REPO = '[YOUR-REPO]'\n",
    "# HUGGINGFACE_AUTH_TOKEN = '[YOUR-TOKEN]' # https://huggingface.co/settings/token\n",
    "\n",
    "# # Push to huggingface-hub\n",
    "# model.push_to_hub(\n",
    "#     MODEL_SAVE_REPO,\n",
    "#     use_temp_dir=True,\n",
    "#     use_auth_token=HUGGINGFACE_AUTH_TOKEN\n",
    "# )\n",
    "\n",
    "# tokenizer.push_to_hub(\n",
    "#     MODEL_SAVE_REPO,\n",
    "#     use_temp_dir=True,\n",
    "#     use_auth_token=HUGGINGFACE_AUTH_TOKEN\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
