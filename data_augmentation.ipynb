{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "LABELS_DIR = \"labels\"\n",
    "DATA_DIR = \"data\"\n",
    "AUG_DIR = \"aug\"\n",
    "\n",
    "if path.exists(AUG_DIR):\n",
    "    os.rmdir(AUG_DIR)\n",
    "\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    for dir in dirs:\n",
    "        new_root = f\"{AUG_DIR}/{root[5:]}/{dir}\"\n",
    "        if not path.exists(new_root):\n",
    "           os.makedirs(new_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.io as io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, labels_dir, transform, device=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.df = self.build_df()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text\n",
    "        file_name = self.df[\"file_name\"][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        img = io.read_image(\n",
    "            path.join(self.dataset_dir, file_name))\n",
    "        if self.device:\n",
    "            img.to(self.device)\n",
    "        return (file_name, self.transform(img))\n",
    "\n",
    "    def build_df(self):\n",
    "        li = []\n",
    "        for root, dirs, files in os.walk(self.labels_dir):\n",
    "            for file in files:  # Loop through the dataset tsvfiles\n",
    "                if not file.endswith(\".tsv\"):\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing {file}\")\n",
    "                li.append(pd.read_table(path.join(root, file),\n",
    "                          names=[\"file_name\", \"text\"]))\n",
    "\n",
    "        return pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dotin13/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dotin13/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing digit_95k.tsv\n",
      "Processing hand_line_all_548k.tsv\n",
      "Processing tang_syn_1577k.tsv\n",
      "Processing web_line_238k.tsv\n",
      "Processing hw_chinese_240k.tsv\n",
      "Processing hwdb_ic13_47k.tsv\n",
      "Processing hwdb2.1_13k.tsv\n",
      "Processing hwdb2.2_12k.tsv\n",
      "Processing hwdb2.0_16k.tsv\n",
      "Processing signatures_472k.tsv\n",
      "Processing hwdb2.0_4k.tsv\n",
      "Processing hwdb_ic13_3k.tsv\n",
      "Processing hw_chinese_17k.tsv\n",
      "Processing hwdb2.2_3k.tsv\n",
      "Processing hwdb2.1_3k.tsv\n",
      "Processing hwdb_ic13_val_5k.tsv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms.v2 import InterpolationMode\n",
    "\n",
    "LABELS_DIR = \"labels\"\n",
    "DATA_DIR = \"data\"\n",
    "AUG_DIR = \"aug\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(\n",
    "        (64, 1024), interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
    "])\n",
    "\n",
    "dataset = OCRDataset(DATA_DIR, LABELS_DIR,\n",
    "                     transform=transform, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n",
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n",
      "  3%|â–Ž         | 373/12879 [03:31<1:58:18,  1.76it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 64, 1024] at entry 0 and [1, 64, 1024] at entry 192",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m data_loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     19\u001b[0m transform \u001b[39m=\u001b[39m build_data_aug((\u001b[39m64\u001b[39m, \u001b[39m1024\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(data_loader):\n\u001b[1;32m     22\u001b[0m     input_images \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\n\u001b[1;32m     23\u001b[0m     transformed_batch \u001b[39m=\u001b[39m transform(input_images\u001b[39m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m samples \u001b[39min\u001b[39;49;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 64, 1024] at entry 0 and [1, 64, 1024] at entry 192"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_aug import build_data_aug\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "LABELS_DIR = \"labels\"\n",
    "AUG_DIR = \"aug\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "transform = build_data_aug((64, 1024), \"train\", device=device)\n",
    "\n",
    "for batch in tqdm(data_loader):\n",
    "    input_images = batch[1]\n",
    "    transformed_batch = transform(input_images.to(device))\n",
    "    for i, item in enumerate(transformed_batch):\n",
    "        img = item\n",
    "        save_image(img, path.join(AUG_DIR, batch[0][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import torchvision.io as io\n",
    "from data_aug import build_data_aug\n",
    "\n",
    "\n",
    "LABELS_DIR = \"labels\"\n",
    "DATA_DIR = \"data\"\n",
    "AUG_DIR = \"aug\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "tfm = build_data_aug(64, \"train\", device=device)\n",
    "\n",
    "for root, dirs, files in os.walk(LABELS_DIR):\n",
    "    for file in files:  # Loop through the dataset tsvfiles\n",
    "        if not file.endswith(\".tsv\"):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {file}\")\n",
    "\n",
    "        with open(path.join(root, file), \"r\", encoding=\"utf-8\") as tsvfile:\n",
    "\n",
    "            for line in tqdm(tsvfile):  # Loop through lines in the tsvfile\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                image_file = line.split(\"\\t\")[0]\n",
    "                image_path = path.join(DATA_DIR, image_file)\n",
    "                if not path.exists(image_path):\n",
    "                    raise Exception(f\"Image file {image_file} not found\")\n",
    "\n",
    "                # aug_dir = path.join(AUG_DIR, \"/\".join(image_file.split(\"/\")[:-1]))\n",
    "                # if not path.exists(aug_dir):\n",
    "                #     os.makedirs(aug_dir)\n",
    "\n",
    "                # input_image = io.read_image(path.join(DATA_DIR, image_file))\n",
    "                # augmented = tfm(input_image.to(device))\n",
    "\n",
    "                # output_path = path.join(AUG_DIR, image_file)\n",
    "                # torchvision.utils.save_image(augmented, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dotin13/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dotin13/mijo/GitHub/hand-syn/tang-syn/venv/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n",
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"V2 Testing\"\"\"\n",
    "\n",
    "from torchvision.transforms import functional\n",
    "\n",
    "import torch\n",
    "import torchvision.io as io\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "from data_aug import build_data_aug\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "input_image = io.read_image(\"0.png\").unsqueeze(0).to(device)\n",
    "tfm = build_data_aug((64, 3072), \"train\", device=device)\n",
    "augmented = tfm(input_image)\n",
    "\n",
    "plt.imshow(functional.to_pil_image(augmented.cpu()[0]))\n",
    "\n",
    "# input_image = input_image.filter(ImageFilter.MinFilter(3))\n",
    "# plt.imshow(input_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"V1 Testing\"\"\"\n",
    "\n",
    "from torchvision.transforms import functional\n",
    "\n",
    "import torch\n",
    "import torchvision.io as io\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "from data_aug import build_data_aug\n",
    "\n",
    "input_image = Image.open(\"0.png\")\n",
    "tfm = build_data_aug(64, \"train\")\n",
    "augmented = tfm(input_image)\n",
    "\n",
    "# plt.imshow(functional.to_pil_image(augmented.cpu()[0]))\n",
    "\n",
    "# plt.imshow(augmented.cpu())\n",
    "\n",
    "plt.imshow(functional.to_pil_image(augmented))\n",
    "\n",
    "# input_image = input_image.filter(ImageFilter.MinFilter(3))\n",
    "# plt.imshow(input_image)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
